
With the arrival of virtualization and cloud computing it was evident that
the world of information technology needed to change the way to use its
resources. Information technology has been since years one of the fastest
growing market in the world and it is evident how, in just a couple of
years, we passed from running services on bare-metal servers to a massive
use of virtualization. Moreover, just a few years after virtualization
became popular, all the attention shifted on container platforms like
Docker.

The point is that people working in Information Technology need, now more than
ever, to stay constantly updated on what the market and the community proposes. They need to constantly evolve and improve their infrastructure with an high
frequency.

Consequentially, even the way servers are managed and how applications are
written has changed noticeably in the past years. Applications are built
in a way to consider all their resources as unstable and unreliable: they
are instructed by the programmer on how to deal with failures.
Architectures and services with one hundred percent uptime are a dream for
all engineers but the truth is that software and hardware can, and sooner
or later will, fail. The only way to prevent our clients not to see the
unreliability of the physical infrastructure is to hide errors, plan
redundancy, make our product ready for a eventual failure.

A new concept called "Designing for failure" changed the prospective about
how to manage services. Before engineers had to plan the infrastructure
trying to eliminate all the possibility of failures. This approach has
proven to be difficult to handle and to support since failures are
inevitable end very difficult to predict, especially when the scale of the
infrastructure grows.

The new mentality changes its focus: since failures are unavoidable it is
useless to put all the effort trying to prevent them, instead we should
instruct our software to deal with failures while keeping the service
running. This mentality guided software developers and engineers into
coding more reliable software, that knows that its resources might not be
stable all the time.

This work of thesis will analyse how this mentality shift changed the
classical infrastructure that we used to have. Furthermore we will
describe and study experimentally how CERN uses this new approach to build
new services which now offer higher uptime and improved stability.
Focusing on the CERN data centre, we will study the improvements made and
also the problems that this new approach can lead to. We will focus then
on the problem of configuration drifts, trying to find a solution to
address this issue. The thesis describes the approach taken in order to
detect and reduce the number of drifts with the development of two tools.
Finally we will report some conclusion on how those softwares have been
used to address some problems verified.

